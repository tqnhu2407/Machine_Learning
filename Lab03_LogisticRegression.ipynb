{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Lab03-LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrdL_pEeIHZX"
      },
      "source": [
        "# Lab03: Logistic Regression.\n",
        "\n",
        "- Student ID: 18127266\n",
        "- Student name: Trần Quỳnh Như"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMXFgGG-IHZm"
      },
      "source": [
        "**How to do your homework**\n",
        "\n",
        "\n",
        "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
        "\n",
        "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
        "\n",
        "**How to submit your homework**\n",
        "\n",
        "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
        "\n",
        "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`). Copy file notebook to this folder, compress and submit it on moodle.\n",
        "\n",
        "**Contents:**\n",
        "- Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDVg0Sz6IHZo"
      },
      "source": [
        "## 1. Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m1Xuc_FIHZo"
      },
      "source": [
        "### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lAEK49LIHZp"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DchgP2pxIHZr"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOkwPiuJIHZs"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', return_X_y=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMD6xx6eIHZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d117584c-7a8e-41f7-cd8c-3a07b60d6c2e"
      },
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000, 784)\n",
            "(70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV9rgS9lIHZu"
      },
      "source": [
        "### Extract Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGeADmvJIHZv"
      },
      "source": [
        "So we basically have 70000 samples with each sample having 784 features - pixels in this case and a label - the digit the image represent.\n",
        "\n",
        "Let’s play around and see if we can extract any features from the pixels that can be more informative. First I’d like to know more about average intensity - that is the average value of a pixel in an image for the different digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9M-JoKaIHZw"
      },
      "source": [
        "labels=np.unique(y)\n",
        "# print(labels)\n",
        "n_label=np.unique(y).shape[0]\n",
        "l_means=np.zeros(shape=n_label,dtype=float) #array stores average intensity for each label\n",
        "\n",
        "#TODO compute average intensity for each label\n",
        "n_frequency = np.zeros(shape=10,dtype=int)\n",
        "\n",
        "for i in range(70000):\n",
        "    each_label = int(y[i])\n",
        "    l_means[each_label] += np.mean(X[i])\n",
        "    n_frequency[each_label] += 1\n",
        "\n",
        "l_means = l_means / n_frequency\n",
        "\n",
        "# print(l_means)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQpzBMagIHZw"
      },
      "source": [
        "Plot the average intensity using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZkg5qQMIHZx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "c74de0a0-00fb-4325-976e-55ce83bb85e2"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(labels,l_means)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWLmbZbrIHZx"
      },
      "source": [
        "As we can see there are some differences in intensity. The digit “1” is the less intense while the digit “0” is the most intense. So this new feature seems to have some predictive value if you wanted to know if say your digit is a “1” or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_zjmBiXIHZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e677e0bc-9fc9-491e-9770-4fec73878991"
      },
      "source": [
        "#TODO compute average intensity for each data sample\n",
        "# intensity=?\n",
        "intensity = np.zeros(shape=70000,dtype=float) #array stores average intensity for each label\n",
        "for i in range(70000):\n",
        "    intensity[i] = np.mean(X[i])\n",
        "    \n",
        "print(intensity.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNzarifWIHZy"
      },
      "source": [
        "Sometimes people really do not know what are they doing. I am not an exception:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1UHjrdBIHZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7fa4b98-90aa-4f73-b6a7-83eac1a12333"
      },
      "source": [
        "X_flip=np.flip(X)\n",
        "symmetry= np.mean((X-X_flip),axis=1)\n",
        "print(symmetry.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONxV3LA-IHZz"
      },
      "source": [
        "I called this feature \"symmetry\" (though it's not \"symmetry\" at all). Use visualization method to understand why this thing works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLbHTvOvIHZ0"
      },
      "source": [
        "Our new trainning data will have 70000 samples and 2 features: intensity, symmetry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aH51f4VIHZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b07ab3-3d8b-4183-dcee-8aaad93da630"
      },
      "source": [
        "#TODO create X_new by horizontal stack intensity and symmetry\n",
        "X_new = np.stack((intensity, symmetry), axis=1)\n",
        "\n",
        "print(X_new.shape) #it should be (70000,2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUq_5cFaIHZ1"
      },
      "source": [
        "## 2. Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yheAXlPhIHZ1"
      },
      "source": [
        "Usually logistic regression is a good first choice for classification. In this homework we use logistic regression for classifying digit 1 images and not digit 1 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE6tJVK0IHZ1"
      },
      "source": [
        "### Normalize data\n",
        "First normalize data using Z-score normalization\n",
        "- **TODO: Study about Z-score normalization**\n",
        "- **TODO: Why should we normalize data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOoEoVoy2DzR"
      },
      "source": [
        "**Study about Z-score normalization**\n",
        "\n",
        "- Z-score is one of the most common Normalization techniques.\n",
        "\n",
        "- it represents the number of standard deviations away from the mean.\n",
        "\n",
        "- It ensures that our feature distributions have **mean** $\\mu = 0$ and **standard deviation** $\\sigma = 1$.\n",
        "\n",
        "- It's advantageous when there are a few outliers but clipping is not so neccessary.\n",
        "\n",
        "Formula for calculating the z-score:\n",
        "\n",
        "$$\\dfrac{x-\\mu}{\\sigma}$$\n",
        "\n",
        "Reference: https://developers.google.com/machine-learning/data-prep/transform/normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QiOZbRx1xfB"
      },
      "source": [
        "**Why should we normalize data?**\n",
        "\n",
        "- Data normalizing is a significant part of Machine Learning.\n",
        "\n",
        "- In the data set, all the features may have different ranges of value, or they are on different scales. \n",
        "\n",
        "- *For example*, I did a survey to collect information of different people. The feature Age's range is from about 18 - 60, but the feature Income's range might be from 100 - 10,000 (dollar).\n",
        "\n",
        "- With the example above, we can conclude that **if we forget to normalize, one of the features in our data set can dominate the others**.\n",
        "\n",
        "Reference: https://www.codecademy.com/articles/normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEKWwm5GIHZ2"
      },
      "source": [
        "#TODO: normalize X_new\n",
        "\n",
        "muy = np.mean(X_new, axis=0)\n",
        "standard_deviation = np.std(X_new, axis=0)\n",
        "\n",
        "X_new = (X_new - muy) / standard_deviation"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dNCJGb6IHZ2"
      },
      "source": [
        "### Construct  data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NexNq3VIHZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ea444f-7e0a-45ed-ece6-f9ce1a8b7699"
      },
      "source": [
        "X_new = np.hstack((np.ones((len(X_new), 1)), X_new)) #stack 1s column as usual\n",
        "y_new=y.astype(int)\n",
        "y_new[y_new != 1] = 0 # digit 1 -> class 1, other digits -> class 0\n",
        "y_new=y_new.reshape(-1,1)\n",
        "print (X_new.shape)\n",
        "print (y_new.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000, 3)\n",
            "(70000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37V1nhNIHZ3"
      },
      "source": [
        "### Split data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkIgo-DZIHZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417dec03-34f9-42bf-dd02-25a35d1accc3"
      },
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(X_new, y_new, test_size= int(1/3*X.shape[0]))\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(46667, 3)\n",
            "(46667, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkkT9EqIHZ4"
      },
      "source": [
        "### Sigmoid function and derivative of the sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qa5zA8YIHZ5"
      },
      "source": [
        "def sigmoid_activation(x):\n",
        "    \"\"\"compute the sigmoid activation value for a given input\"\"\"\n",
        "    return 1.0 / (1 + np.exp(-x))\n",
        "def sigmoid_deriv(x):\n",
        "    '''compute the derivative of the sigmoid function ASSUMING\n",
        "    that the input ‘x‘ has already been passed through the sigmoid\n",
        "    activation function'''\n",
        "    return x * (1 - x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C4iB_rkIHZ5"
      },
      "source": [
        "### Compute output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy6BrMKPIHZ6"
      },
      "source": [
        "def compute_h(W, X):\n",
        "    \"\"\"\n",
        "    Compute output: Take the dot product between our features ‘X‘ and the weight\n",
        "    matrix ‘W‘, then pass this value through our sigmoid activation function \n",
        "    \"\"\"\n",
        "    return sigmoid_activation(X.dot(W))\n",
        "def predict(W, X):\n",
        " \n",
        "    '''Take the dot product between our features and weight matrix, \n",
        "       then pass this value through our sigmoid activation'''\n",
        "    #........\n",
        "    preds=sigmoid_activation(X.dot(W))\n",
        "    # apply a step function to threshold the outputs to binary\n",
        "    # class labels\n",
        "    preds[preds <= 0.5] = 0\n",
        "    preds[preds > 0] = 1\n",
        "\n",
        "    return preds"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9POwy-FfIHZ6"
      },
      "source": [
        "### Compute gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRe7PtPAIHZ7"
      },
      "source": [
        "***italicized text* Loss Function: Average negative log likelihood**\n",
        "$$\\mathcal{L}=\\dfrac{1}{N} \\sum_{i=1}^{N} -\\left(y^{i}\\ln h_{\\mathbf{w}}\\left(\\mathbf{x}^{i}\\right)+\\left(1-y^{i}\\right)\\ln \\left(1-h_{\\mathbf{w}}\\left(x^{i}\\right)\\right)\\right) $$\n",
        "\n",
        "\n",
        "$$\\text{Sigmoid Activation: } z= \\sigma \\left(h\\right)= \\dfrac{1}{1+e^{-h}}$$\n",
        "\n",
        "$$\\text{Cross-entropy: } J(w)=-\\left({ylog(z)+(1-y)log(1-z)}\\right)$$\n",
        "\n",
        "$$\\text{Chain rule: } \\dfrac{\\partial J(w)}{\\partial w}=\\dfrac{\\partial J(w)}{\\partial z} \\dfrac{\\partial z}{\\partial h}\\dfrac{\\partial h}{\\partial w}  $$\n",
        "\n",
        "$$\\dfrac{\\partial J(w)}{\\partial z}=-\\left(\\dfrac{y}{z}-\\dfrac{1-y}{1-z}\\right)=\\dfrac{z-y}{z(1-z)}$$\n",
        "\n",
        "$$\\dfrac{\\partial z}{\\partial h}=z(1-z)$$\n",
        "\n",
        "$$\\dfrac{\\partial h}{\\partial w}=X$$\n",
        "\n",
        "$$\\dfrac{\\partial J(w)}{\\partial w}=X^T(z-y)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClsoqY98IHZ8"
      },
      "source": [
        "def compute_gradient(X, error):\n",
        "    \"\"\"\n",
        "    This is the gradient descent update of \"average negative loglikelihood\" loss function. \n",
        "    In lab02 our loss function is \"sum squared error\".\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "    gradient = X.T.dot(error)\n",
        "    return gradient"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PWrfUa9IHZ9"
      },
      "source": [
        "def train(W,train_X, train_y, learning_rate, num_epochs, losses):\n",
        "    for epoch in np.arange(0, num_epochs):\n",
        "        h=compute_h(W,train_X)\n",
        "        error = h - train_y\n",
        "        loss = np.mean(- train_y * np.log(h) - (1 - train_y) * np.log(1 - h))\n",
        "        losses.append(loss)\n",
        "        gradient=compute_gradient(train_X, error)\n",
        "        W += -learning_rate * gradient\n",
        "        if ((epoch+1)%1000==0): print ('Epoch %d, loss %.3f' %(epoch+1, loss))\n",
        "        \n",
        "    return W"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lg9DTLOIHZ-"
      },
      "source": [
        "### Train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Ijv4VzTQIHaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848f1e75-b3e8-4361-f3a8-0fed1ddb2875"
      },
      "source": [
        "W = np.random.randn(train_X.shape[1], 1)\n",
        "losses=[]\n",
        "num_epochs=40000\n",
        "learning_rate=0.01\n",
        "W=train(W,train_X, train_y, learning_rate, num_epochs , losses)\n",
        "x_preds=predict(W ,train_X)\n",
        "train_err = np.mean(x_preds != train_y) * 100\n",
        "print ('=' * 50)\n",
        "print ('Train err of final w: ', train_err)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1000, loss nan\n",
            "Epoch 2000, loss nan\n",
            "Epoch 3000, loss nan\n",
            "Epoch 4000, loss nan\n",
            "Epoch 5000, loss nan\n",
            "Epoch 6000, loss nan\n",
            "Epoch 7000, loss nan\n",
            "Epoch 8000, loss nan\n",
            "Epoch 9000, loss nan\n",
            "Epoch 10000, loss nan\n",
            "Epoch 11000, loss nan\n",
            "Epoch 12000, loss nan\n",
            "Epoch 13000, loss nan\n",
            "Epoch 14000, loss nan\n",
            "Epoch 15000, loss nan\n",
            "Epoch 16000, loss nan\n",
            "Epoch 17000, loss nan\n",
            "Epoch 18000, loss nan\n",
            "Epoch 19000, loss nan\n",
            "Epoch 20000, loss nan\n",
            "Epoch 21000, loss nan\n",
            "Epoch 22000, loss nan\n",
            "Epoch 23000, loss nan\n",
            "Epoch 24000, loss nan\n",
            "Epoch 25000, loss nan\n",
            "Epoch 26000, loss nan\n",
            "Epoch 27000, loss nan\n",
            "Epoch 28000, loss nan\n",
            "Epoch 29000, loss nan\n",
            "Epoch 30000, loss nan\n",
            "Epoch 31000, loss nan\n",
            "Epoch 32000, loss nan\n",
            "Epoch 33000, loss nan\n",
            "Epoch 34000, loss nan\n",
            "Epoch 35000, loss nan\n",
            "Epoch 36000, loss nan\n",
            "Epoch 37000, loss nan\n",
            "Epoch 38000, loss nan\n",
            "Epoch 39000, loss nan\n",
            "Epoch 40000, loss nan\n",
            "==================================================\n",
            "Train err of final w:  11.09134934750466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLtJHGsVIHaB"
      },
      "source": [
        "## 3. Evaluate our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c13j4E1oIHaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd071cf3-5844-47b0-efb7-d8987377fbf5"
      },
      "source": [
        "preds = predict(W, train_X)\n",
        "print(classification_report(train_y, preds))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94     41358\n",
            "           1       0.91      0.03      0.05      5309\n",
            "\n",
            "    accuracy                           0.89     46667\n",
            "   macro avg       0.90      0.51      0.50     46667\n",
            "weighted avg       0.89      0.89      0.84     46667\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKToYeFVIHaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a586947f-941a-4c73-bc0e-c3675b84457d"
      },
      "source": [
        "preds = predict(W, test_X)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94     20765\n",
            "           1       0.89      0.03      0.06      2568\n",
            "\n",
            "    accuracy                           0.89     23333\n",
            "   macro avg       0.89      0.52      0.50     23333\n",
            "weighted avg       0.89      0.89      0.85     23333\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp9wkzXBIHaD"
      },
      "source": [
        "**TODO: Comment on the result**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfqsGsrvsjxW"
      },
      "source": [
        "Generally, the classification reports of the 2 models are nearly the same.\n",
        "\n",
        "The evaluated *accuracy* of the 2 models is identical (**89%**), which is a pretty high proportion.\n",
        "\n",
        "In addition, the *macro avg* an *weighted avg* of the 2 models have approximate *precisions*, *recalls* and *f1-scores* values."
      ]
    }
  ]
}